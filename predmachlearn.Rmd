---
title: "Practical Machine Learning, Course Project"
author: "Aditya Salapaka"
date: "Friday 20 June 2015"
output: html_document
---

# Practical Machine Learning, Course Project

## Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible
to collect a large amount of data about personal activity relatively 
inexpensively. These type of devices are part of the quantified self movement â€“ 
a group of enthusiasts who take measurements about themselves regularly to 
improve their health, to find patterns in their behaviour, or because they are 
tech geeks. One thing that people regularly do is quantify how much of a 
particular activity they do, but they rarely quantify how well they do it. 

Six young health participants were asked to perform one set of 10 repetitions of
the Unilateral Dumbbell Biceps Curl in five different fashions:

* Class A: exactly according to the specification
* Class B: throwing the elbows to the front
* Class C: lifting the dumbbell only halfway
* Class D: lowering the dumbbell only halfway
* Class E: throwing the hips to the front

The aim is to predict the manner in which the participants did the exercise.

## Overview

### Model Selection
Since the data is non-linear and interaction of all variables has to be 
considered, thee Decision Tree and Random Forest Machine Learning algorithms fit
our requirements. Of course, the Random Forest method will give a better 
accuracy during prediction, but also takes more time. This project will also 
conclude the degree to which prediction using Random Forest is superior to
Decision Tree. 

### Cross-validation and Out-of-sample Error

The training dataset has 19622 observations, and the sheer number of records 
enable us to sub-sample the training dataset further into subtraining and 
subtesting datasets. These sets are used for cross-validation. The errors that
occur in trees are misclassification errors and are estimated using the accuracy.

## Data

The training data for this project are available [here](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv).

The test data are available [here](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)

## Required Libraries

The following libraries are required for this project:

```{r libraries, message=FALSE}
library(caret)
library(rpart)
library(rattle)
library(randomForest)
```

## Getting Data

The training and test data sets were downloaded from the links given above. 
They were saved in the working directory with their default file names.

A quick overview of both datasets showed that NA values were registered as 
either `"NA"`, `"#DIV/0!"` or `""`. This argument was used in loading both the
datasets into the workspace.

```{r loading}
testing <- read.csv("pml-testing.csv",  na.strings = c("NA", "#DIV/0!", ""))
training <- read.csv("pml-training.csv", na.strings = c("NA", "#DIV/0!", ""))
```

## Cleanup

Checking for the number of variables and rows in both sets:

```{r dim}
dim(testing)
dim(training)
```

Both sets have 160 variables, currently. 

From a glance at the training dataset, we see that the first seven variables
are unnecessary from a machine learning standpoint. These are eliminated from
both datasets.

```{r remove 1:7}
training <- training[-(1:7)]
testing <- testing[-(1:7)]
```

Further, there are some variables which contain no data at all, that is, are 
completely filled with NA values. These are also removed from the datasets.

```{r remove NA variables}
training <- training[,colSums(is.na(training)) == 0]
testing <- testing[,colSums(is.na(testing)) == 0]
```

Now, we need to eliminate the variables which cause nearly zero variance.

```{r zerovar}
nsv <- nearZeroVar(training, saveMetrics = TRUE)
sum(nsv$nzv)
```

We see that no variables provide near zero variance.

## Subsampling

The training set available is split into another training and test set, in a 
70:30 ratio for cross-validation purposes.

A seed of 4200 is set for the sake of reproducibility. 

```{r subsampling}
set.seed(4200)

inTrain <- createDataPartition(y = training$classe, p = 0.7, list = FALSE)
subtraining <- training[inTrain,]
subtesting <- training[-inTrain,]
```

## Prediction Using Machine Learning Algorithms

### Decision Tree


```{r tree}
treeFit <- rpart(classe ~ ., method = "class", data = subtraining)
```

This is the decision tree generated.

```{r tree plot}
fancyRpartPlot(treeFit, main = "Decision Tree", 
               sub = "Rpart Decision Tree To Predict Classe")
```

Using the model to predict in the test sub-sample:

```{r tree predict}
treePredict <- predict(treeFit, subtesting, type = "class")
```

The accuracy of the prediction is checked using a confusion matrix.

```{r tree confusion matrix}
confusionMatrix(treePredict, subtesting$classe)
```

We obtain an accuracy of 73.81.

### Random Forest

```{r forest}
forestFit <- randomForest(classe ~ ., data = subtraining, method = "class")
```

Using the model to predict in the test sub-sample:

```{r forest predict}
forestPredict <- predict(forestFit, subtesting, type = "class")
```

The accuracy of the prediction is checked using a confusion matrix.

```{r forest confusion matrix}
confusionMatrix(forestPredict, subtesting$classe)
```

We obtain an accuracy of 99.51.

## Conclusion

The Random Forest algorithm predicts with an accuracy of 99.51, which is much
more superior than the accuracy of the Decision Tree algorithm, which is 73.81.
Of course, the accuracy in a larger test set will be lower than this, but will
still be high enough to guarantee an accurate prediction. In fact, our testing
dataset has only 20 cases, so this high accuracy should predict correctly.

## Submission

The final prediction on the testing dataset is:

```{r final prediction}
prediction <- predict(forestFit, testing, type = "class")
prediction
```

A function is used to generate the files for submission.

```{r generating files}
pml_write_files = function(x){
        n = length(x)
        for(i in 1:n){
                filename = paste0("problem_id_",i,".txt")
                write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
        }
}

pml_write_files(prediction)
```