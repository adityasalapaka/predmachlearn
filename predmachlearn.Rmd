# Practical Machine Learning, Course Project

## Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible
to collect a large amount of data about personal activity relatively 
inexpensively. These type of devices are part of the quantified self movement â€“ 
a group of enthusiasts who take measurements about themselves regularly to 
improve their health, to find patterns in their behavior, or because they are 
tech geeks. One thing that people regularly do is quantify how much of a 
particular activity they do, but they rarely quantify how well they do it. In 
this project, your goal will be to use data from accelerometers on the belt, 
forearm, arm, and dumbell of 6 participants. They were asked to perform barbell 
lifts correctly and incorrectly in 5 different ways. More information is 
available from the website [here](http://groupware.les.inf.puc-rio.br/har]).

## Data

The training data for this project are available [here](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv).

The test data are available [here](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)

The goal of this project is to predict the manner in which the participants did
the exercise. The `classe` variable in the data set has five factors:

* A = 
* B = 
* C =
* D = 
* E = 

## Required Libraries

The following libraries are required for this project:

```{r libraries, message=FALSE}
library(caret)
library(rpart)
library(rattle)
library(randomForest)
```

## Getting Data

The training and test data sets were downloaded from the links given above. 
They were saved in the working directory with their default filenames.

A quick overview of both datasets showed that NA values were registered as 
either `"NA"`, `"#DIV/0!"` or `""`. This argument was used in loading both the
datasets into the workspace.

```{r loading}
testing <- read.csv("pml-testing.csv",  na.strings = c("NA", "#DIV/0!", ""))
training <- read.csv("pml-training.csv", na.strings = c("NA", "#DIV/0!", ""))
```

## Cleanup

Checking for the number of variables and rows in both sets:

```{r dim}
dim(testing)
dim(training)
```

Both sets have 160 variables, currently. 

From a glance at the training dataset, we see that the first seven variables
are unnecessary from a machine learning standpoint. These are eliminated from
both datasets.

```{r remove 1:7}
training <- training[-(1:7)]
testing <- testing[-(1:7)]
```

Further, there are some variables which contain no data at all, that is, are 
completely filled with NA values. These are also removed from the datasets.

```{r remove NA variables}
training <- training[,colSums(is.na(training)) == 0]
testing <- testing[,colSums(is.na(testing)) == 0]
```

Now, we need to eliminate the variables which cause nearly zero variance.

```{r zerovar}
nsv <- nearZeroVar(training, saveMetrics = TRUE)
sum(nsv$nzv)
```

We see that no variables provide near zero variance.

## Subsampling

The training set available is split into another training and test set, in a 
70:30 ratio for cross-validation purposes.

A seed of 4200 is set for the sake of reproducibility. 

```{r subsampling}
set.seed(4200)

inTrain <- createDataPartition(y = training$classe, p = 0.7, list = FALSE)
subtraining <- training[inTrain,]
subtesting <- training[-inTrain,]
```

## Prediction Using Machine Learning Algorithms

### Decision Tree


```{r tree}
treeFit <- rpart(classe ~ ., method = "class", data = subtraining)
```

This is the decision tree generated.

```{r tree plot}
fancyRpartPlot(treeFit, main = "Decision Tree", 
               sub = "Rpart Decision Tree To Predict Classe")
```

Using the model to predict the test subsample:

```{r tree predict}
treePredict <- predict(treeFit, subtesting, type = "class")
```

The accuracy of the prediction is checked using a confusion matrix.

```{r tree confusion matrix}
confusionMatrix(treePredict, subtesting$classe)
```

We obtain an accuracy of 73.81.